# **大数据常见问题解决方案**
***
##1、在非局域网中配置Hadoop集群ip，外节点使用公网ip，本节点使用私有ip。
##2、在配置hadoop2.3.0集群时，在yarn-site.xml配置文件中需要显示指定resourcemanager的相关信息，否则即使各个节点上启动了nodemanager，也会因为找不到resourcemanager而去连接0.0.0.0的ip的相关yarn的80xx端口而报错，在8088的web页面上看到的集群内存分配也是0（即使在配置文件中已经配置），需要配置的信息如下：
> `<property>`  
  `<name>yarn.resourcemanager.resource-tracker.address</name>`  
  `<value>your_rm'host:8031</value>`  
`</property>`  
`<property>`  
  `<name>yarn.resourcemanager.address</name>`  
  `<value>your_rm'host:8032</value>`  
`</property>`  
`<property>`  
  `<name>yarn.resourcemanager.scheduler.address</name>`  
  `<value>your_rm'host:8030</value>`  
`</property>`  
`<property>`  
  `<name>yarn.resourcemanager.admin.address</name>`  
  `<value>your_rm'host:8033</value>`  
`</property>`  
`<property>`  
  `<name>yarn.resourcemanager.webapp.address</name>`  
  `<value>your_rm'host:8088</value>`  
`</property>`  
##3、配置hive的mysql matestores时，需要先初始化数据库
>`schematool -dbType mysql -initSchema`  
>（重新初始化前需要先删除namenode和datanode中的data）
##4、在hive中使用beeline、datagrip等时，需要
>1）在core-site.xml配置：  
        `<property>`  
               ` <name>hadoop.proxyuser.zrf.hosts</name>`  
                `<value>*</value>`  
        `</property>`  
        `<property>`  
                `<name>hadoop.proxyuser.zrf.groups</name>`  
                `<value>*</value>`  
        `</property>`  
2）在hdfs-site.xml中配置：  
        `<property>`  
                `<name>dfs.webhdfs.enabled</name>`  
                `<value>true</value>`  
        `</property>`  
##6、双亲委派机制  
##7、乱码问题
>`byte[] bs = s.getByted("ISO8859-1")`  
>`String newString = new String(bs,"UTF-8")`  
##8、什么时候应该启动hiveserver2？
>当通过jdbc连接hive时需要，比如beeline、一些可视化客户端。  
##9、什么时候启动metestore？
>当需要获取元数据时（获得元数据两种方式：客户端支链、启动metestore）
##10、使用sqoop从MySQL向hdfs导入数据失败？
>可能是hdfs的安全模式正在启动中  
>（可手动离开安全模式`hdfs dfsadmin -safemode leave`）
##11、使用脚本ssh启动hadoop104的消费flume，进程启动成功但不起作用，必须在控制台直接输入命令？
> 创建/etc/profile.d/my_env.sh 文件，在其中添加环境变量  
>解释：login shell启动时会加载/etc/profile。non-login shell启动时会加载~/.bashrc。但是无论在加载~/.bashrc（实际上是加载了~/.bashrc中的/etc/bashrc）或/etc/profile时，都会执行/etc/profile.d/*.sh，所以，无论在login shell或non-login shell环境中，都会加载/etc/profile.d/*.sh文件，这样我们可以自定义一个my_env.sh文件用来存放java或者其他的环境变量，一劳永逸！
##12、hive on spark执行job时spark会话不能创建？
>查看hive的两个配置文件的路径、端口号是否错误
